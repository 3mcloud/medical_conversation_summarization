{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e85c6eb",
   "metadata": {},
   "source": [
    "#### Example scripts on various types of evaluation\n",
    "\n",
    "**NOTE**:\n",
    "- *eval_umls()*: UMLS concept evaluation requires *quickUMLS* library and *UMLS* database. See [this repo](https://github.com/Georgetown-IR-Lab/QuickUMLS) for installation details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import utils.data_evaluation as DataEval\n",
    "import imp\n",
    "imp.reload(DataEval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mean-of-mean and mean-of-best Rouge evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def process_fn(text):\n",
    "    # define your own processing function that further modifies BOTH conversation and summary texts\n",
    "    raise NotImplementedError\n",
    "    \n",
    "exp = \"experiment_name\"\n",
    "ref_file = '../../data/test.jsonl' # original data file\n",
    "meta_files = [ \n",
    "    f'../../experiments/{exp}/test_stage2.meta',\n",
    "] # string variable of a single meta file or a list of meta file strings\n",
    "hypo_files = [\n",
    "    f'../../experiments/{exp}/test_stage2.hypo',\n",
    "] # list of hypo files\n",
    "\n",
    "df, mom, mob = DataEval.eval_rouge(hypo_files, meta_files, ref_file, process_fn=None, remove_duplicate=True)\n",
    "pd.DataFrame(mom)  # mean-of-mean score matrix\n",
    "pd.DataFrame(mob)  # mean-of-best score matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77a4b86",
   "metadata": {},
   "source": [
    "umls evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd22933",
   "metadata": {},
   "outputs": [],
   "source": [
    "umls_matcher = DataEval.config_UMLSmatcher(path='path_to_UMLS_database')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9e261a",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = \"experiment_name\"\n",
    "ref_file = '../../data/test.jsonl' # original data file\n",
    "meta_files = [ \n",
    "    f'../../experiments/{exp}/test_stage2.meta',\n",
    "] # string variable of a single meta file or a list of meta file strings\n",
    "hypo_files = [\n",
    "    f'../../experiments/{exp}/test_stage2.hypo',\n",
    "] # list of hypo files\n",
    "\n",
    "df, Mumls = DataEval.eval_umls(hypo_files, meta_files, ref_file, matcher=umls_matcher, remove_duplicate=True)\n",
    "for x in Mumls:\n",
    "    with np.printoptions(precision=3):\n",
    "        print(x.mean(axis=0))  # F1, P, R\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rouge evaluation among references\n",
    "\n",
    "calculate rouge score among multiple references, using one reference at a time as hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5570b9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mom, mob = DataEval.eval_ref('../../data/example.jsonl', process_fn=None)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8e4853d7aa76789e0fea06d9b8164f7bc1c074c36b35b3afef548aec61595048"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('pytorch': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
